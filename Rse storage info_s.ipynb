{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df0e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, collect_list, collect_set, concat_ws, first, format_number, from_unixtime, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    split as _split,\n",
    "    sum as _sum,\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    "    DecimalType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f285775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8e39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = datetime.today().strftime('%Y-%m-%d')\n",
    "# Rucio\n",
    "\n",
    "HDFS_RUCIO_CONTENTS = f\"/project/awg/cms/rucio/{TODAY}/contents/part*.avro\"\n",
    "HDFS_RUCIO_DATASET_LOCKS = f'/project/awg/cms/rucio/{TODAY}/dataset_locks/part*.avro'\n",
    "HDFS_RUCIO_DIDS = f\"/project/awg/cms/rucio/2022-07-17/dids/part*.avro\"\n",
    "HDFS_RUCIO_LOCKS = f\"/project/awg/cms/rucio/{TODAY}/locks/part*.avro\"\n",
    "HDFS_RUCIO_REPLICAS = f\"/project/awg/cms/rucio/{TODAY}/replicas/part*.avro\"\n",
    "HDFS_RUCIO_RSES = f'/tmp/cmsmonit/rucio_daily_stats-{TODAY}/RSES/part*.avro'\n",
    "HDFS_RUCIO_RULES = f\"/project/awg/cms/rucio/{TODAY}/rules/part*.avro\"\n",
    "\n",
    "# DBS\n",
    "HDFS_DBS_DATASETS = f'/tmp/cmsmonit/rucio_daily_stats-{TODAY}/DATASETS/part*.avro'\n",
    "HDFS_DBS_BLOCKS = f'/tmp/cmsmonit/rucio_daily_stats-{TODAY}/BLOCKS/part*.avro'\n",
    "HDFS_DBS_FILES = f'/tmp/cmsmonit/rucio_daily_stats-{TODAY}/FILES/part*.avro'\n",
    "TB_DENOMINATOR = 10 ** 12\n",
    "FLT_N_TB_DGTS = 8\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68959457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_locks(spark):\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_LOCKS) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "        .withColumn('f_size', col('BYTES').cast(LongType())) \\\n",
    "        .filter(col('state')=='O')\\\n",
    "        .withColumnRenamed('NAME', 'f_name') \\\n",
    "        .select(['f_name', 'rse_id', 'f_size'])\n",
    "\n",
    "\n",
    "#         .filter(col('state')=='A')\\\n",
    "#These include replicas that are unavailable too\n",
    "def get_df_replicas(spark):\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_REPLICAS) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "        .withColumn('f_size', col('BYTES').cast(LongType())) \\\n",
    "        .withColumn('locked', when((col(\"lock_cnt\") > 0), 'y').otherwise('n')) \\\n",
    "        .select(['rse_id', 'f_size', 'locked', 'lock_cnt'])\n",
    "\n",
    "\n",
    "def get_df_rses(spark):\n",
    "    df_rses = spark.read.format(\"com.databricks.spark.avro\").load(HDFS_RUCIO_RSES) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .withColumn('id', lower(_hex(col('ID')))) \\\n",
    "        .withColumn('rse_tier', _split(col('RSE'), '_').getItem(0)) \\\n",
    "        .withColumn('rse_country', _split(col('RSE'), '_').getItem(1)) \\\n",
    "        .withColumn('rse_kind',\n",
    "                    when((col(\"rse\").endswith('Temp') | col(\"rse\").endswith('temp') | col(\"rse\").endswith('TEMP')),\n",
    "                         'temp')\n",
    "                    .when((col(\"rse\").endswith('Test') | col(\"rse\").endswith('test') | col(\"rse\").endswith('TEST')),\n",
    "                          'test')\n",
    "                    .otherwise('prod')\n",
    "                    ) \\\n",
    "        .select(['id', 'RSE', 'RSE_TYPE', 'rse_tier', 'rse_country', 'rse_kind'])\n",
    "    return df_rses\n",
    "\n",
    "\n",
    "def get_df_rse_limits(spark):\n",
    "    df_rse_limits = pd.read_csv(\"rse_limits.csv\")\n",
    "    df_rse_limits = spark.createDataFrame(df_rse_limits).withColumn('id', lower(col('RSE_ID'))).select([\"id\", \"value\"])\n",
    "    return df_rse_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f26017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replicas = get_df_replicas(spark)\n",
    "df_rses = get_df_rses(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d6cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_by_rse = df_replicas.groupby(\"rse_id\")\\\n",
    "    .pivot(\"locked\", values=['y', 'n'])\\\n",
    "    .agg(func.sum(\"f_size\"), func.count(\"f_size\"))\\\n",
    "    .na.fill(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2443618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409e79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9483c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rse_summary = df_grouped_by_rse.alias(\"rep1\").join(df_rses.alias(\"rse\"), col(\"rep1.rse_id\")==col(\"rse.id\"))\\\n",
    "#     .select([\"rep1.rse_id\", \"rse.rse\", \"rse.rse_type\", \"rep1.count_locked\", \"rep1.size_locked\", \"rep1.count_dynamic\", \"rep1.size_dynamic\", \"rep1.count_total\", \"rep1.size_total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04946c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rse_summary.write.format(\"json\").save(\"rse_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a689a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rse_summary.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", \"rchauhan\").option(\"collection\", \"rse_info_connector\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a2cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rse_summary_pandas = df_rse_summary.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325b0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rse_summary_v2 = df_rse_summary.alias(\"summary\").join(df_rse_limits.alias(\"limits\"), col(\"summary.rse_id\")==col(\"limits.id\"), 'inner')\\\n",
    "#     .withColumnRenamed(\"value\", \"limit\")\\\n",
    "#     .select([\"summary.rse_id\", \"rse\", \"rse_type\", \"rse_tier\", \"rse_country\",\n",
    "#             \"locked_size\", \"locked_file_count\", \"size_not_locked\", \"file_not_locked_count\", \"limit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6199b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rse_summary_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39e4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rse_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d630da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80654acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff76fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3c16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb15722",
   "metadata": {},
   "source": [
    "# Container - File Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe4784",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b35e5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_contents(spark):\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_CONTENTS) \\\n",
    "        .filter(col(\"scope\")==\"cms\")\\\n",
    "        .select(['name', 'child_name', 'did_type', 'child_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0594b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contents = get_df_contents(spark)\n",
    "df_contents_file = df_contents.filter(col(\"child_type\")==\"F\").withColumnRenamed(\"child_name\", \"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ebf9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_file_map = df_contents_file.withColumn(\"dataset\", func.element_at(func.split(\"name\",\"#\"),1)).select([\"file\", \"dataset\"])\n",
    "df_dataset_file_map = df_dataset_file_map.withColumn(\"data_tier\", func.element_at(func.split(\"dataset\",\"/\"),-1)).filter(col(\"data_tier\")!=\"CONTAINER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a3f3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset_file_map.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ef96bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_contents = df_contents.filter(col(\"did_type\")==\"C\").withColumn(\"data_tier\", func.element_at(func.split(\"name\",\"/\"),-1))\n",
    "# df_with_containers = df_contents.select([\"data_tier\"]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47cfc69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_replicas_with_name(spark):\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_REPLICAS) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "        .withColumn('f_size', col('BYTES').cast(LongType())) \\\n",
    "        .withColumn('locked', when((col(\"lock_cnt\") > 0), 'y').otherwise('n')) \\\n",
    "        .filter(col(\"rse_id\").isin(disk_rse_list))\\\n",
    "        .select(['name', 'f_size', 'locked', 'lock_cnt', 'state']) #to get just the disk rses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "591ff74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_replicas_with_name = get_df_replicas_with_name(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7f1841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined = df_replicas_with_name.alias(\"repname1\").join(df_dataset_file_map.alias(\"dfmap1\"), col(\"repname1.name\")==col(\"dfmap1.file\"), \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd1747b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cb9e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #File that do not have a map to a dataset\n",
    "# df_joined.filter(col(\"file\").isNull()).filter(~col(\"name\").contains(\"/store/user\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2960f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_with_unknown = df_joined\\\n",
    "#     .withColumn('data_tier_new', when(col(\"file\").isNull(), \"UNKNOWN\")\\\n",
    "#     .otherwise(col(\"data_tier\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73187f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_with_unknown.filter(col(\"data_tier_new\")==\"UNKNOWN\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef13d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp_new = df_with_unknown.groupby(\"data_tier_new\")\\\n",
    "#     .pivot(\"state\")\\\n",
    "#     .agg(func.sum(\"f_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3773ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp_new2 = df_with_unknown.groupby(\"data_tier_new\")\\\n",
    "#     .pivot(\"locked\")\\\n",
    "#     .agg(func.sum(\"f_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0343621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_tier_state_pivot = df_temp_new.toPandas()\n",
    "# df_data_tier_lock_pivot = df_temp_new2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ad1c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_tier_lock_pivot_copy = df_data_tier_lock_pivot\n",
    "# df_data_tier_state_pivot_copy = df_data_tier_state_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eef05469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_tier_state_pivot.fillna(0, inplace=True)\n",
    "# df_data_tier_state_pivot[\"total\"] = df_data_tier_state_pivot[\"A\"] + df_data_tier_state_pivot[\"B\"] + df_data_tier_state_pivot[\"C\"] + df_data_tier_state_pivot[\"D\"] + df_data_tier_state_pivot[\"U\"]\n",
    "\n",
    "\n",
    "# df_data_tier_state_pivot.sort_values(by=\"total\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3313665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_tier_lock_pivot.to_csv(\"disk_datatier_lock_pivot.csv\")\n",
    "# df_data_tier_state_pivot.to_csv(\"disk_datatier_state_pivot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59fef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_tier_lock_pivot.fillna(0, inplace=True)\n",
    "# df_data_tier_lock_pivot[\"total\"] = df_data_tier_lock_pivot[\"n\"] + df_data_tier_lock_pivot[\"y\"]\n",
    "# df_data_tier_lock_pivot[\"n_tb\"] = df_data_tier_lock_pivot[\"n\"]/(10**12)\n",
    "# df_data_tier_lock_pivot[\"y_tb\"] = df_data_tier_lock_pivot[\"y\"]/(10**12)\n",
    "# df_data_tier_lock_pivot[\"total_tb\"] = df_data_tier_lock_pivot[\"total\"]/(10**12)\n",
    "\n",
    "\n",
    "# df_data_tier_lock_pivot.sort_values(by=\"total\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04d3d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = pd.read_csv(\"data_tier_info_locks.csv\")\n",
    "# temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecffc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files without parent \n",
    "# difference = df_replicas_with_name.select([\"name\"]).distinct().subtract(df_dataset_file_map.select([\"file\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c729d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86bdeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = df_dataset_file_map.groupby(\"file\").agg(collect_set(\"data_tier\").alias(\"tiers\"), collect_list(\"dataset\").alias(\"same_parent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5324e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.filter(func.size(col(\"tiers\")) > 1).filter(~func.array_contains(col(\"tiers\"), \"CONTAINER\")).limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977efd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ab677a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined = df_contents.alias(\"df1\").join(df_contents.alias(\"df2\"), col(\"df1.child_name\")==col(\"df2.name\"), \"inner\" ).select([\"df1.name\", \"df2.child_name\", \"df1.did_type\", \"df2.child_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20bf0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined2 = df_joined.alias(\"df3\").join(df_contents.alias(\"df4\"), col(\"df3.child_name\")==col(\"df4.name\"), \"inner\" ).select([\"df3.name\", \"df4.child_name\", \"df3.did_type\", \"df4.child_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f1fd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined.filter(col(\"child_type\")=='C').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8c367b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map = df_joined.filter(col(\"child_type\")=='F')\\\n",
    "#                         .withColumnRenamed(\"name\", \"cname\")\\\n",
    "#                         .withColumnRenamed(\"child_name\", \"fname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "697f2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map.filter(col(\"did_type\") == \"D\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab598786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map_v2 = df_file_container_map.withColumn(\"data_tier\", func.element_at(func.split(\"cname\",\"/\"),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2f2eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map_v2.groupby(\"data_tier\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9456424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_replicas_with_name(spark):\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_REPLICAS) \\\n",
    "        .withColumn('f_size', col('BYTES').cast(LongType())) \\\n",
    "        .withColumn('locked', when((col(\"lock_cnt\") > 0), 'y').otherwise('n')) \\\n",
    "        .select(['name', 'f_size', 'locked', 'lock_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df242b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_replicas_with_name = get_df_replicas_with_name(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99674854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_replicas_with_name.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "494ab549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = df_file_container_map_v2.alias(\"map1\").join(df_replicas_with_name.alias(\"rep1\"), col(\"map1.fname\")==col(\"rep1.name\"), \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "262d9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_replicas_with_name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4887d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed8cd421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "973ccbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_replicas_with_name.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79e5f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map_v2.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "360ac32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c905522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_contents.filter(col(\"child_type\")==\"F\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad4cd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file_container_map.filter(col(\"child_type\")==\"F\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8804cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_diff = df_contents.filter(col(\"child_type\")==\"F\").select([\"child_name\"]).subtract(df_file_container_map.select([\"fname\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff212594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_diff.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c4be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5805d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50081ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_replicas(spark):\n",
    "    \"\"\"Create main replicas dataframe by selecting only Disk or Tape RSEs in Rucio REPLICAS table\n",
    "\n",
    "    Columns selected:\n",
    "        - f_name: file name\n",
    "        - f_size_replicas: represents size of a file in REPLICAS table\n",
    "        - rse_id\n",
    "        - rep_accessed_at\n",
    "        - rep_created_at\n",
    "    \"\"\"\n",
    "    # List of all RSE id list\n",
    "    # rse_id_list = df_pd_rses['replica_rse_id'].to_list()\n",
    "    # .filter(col('rse_id').isin(rse_id_list)) \\\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_REPLICAS) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "        .withColumn('f_size_replicas', col('BYTES').cast(LongType())) \\\n",
    "        .withColumnRenamed('NAME', 'f_name') \\\n",
    "        .withColumnRenamed('ACCESSED_AT', 'rep_accessed_at') \\\n",
    "        .withColumnRenamed('CREATED_AT', 'rep_created_at') \\\n",
    "        .select(['scope', 'f_name', 'rse_id', 'f_size_replicas', 'rep_accessed_at', 'rep_created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f40fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select count(c.name) from CMS_RUCIO_PROD.replicas r join CMS_RUCIO_PROD.contents c on r.name=c.child_name and r.rse_id = '69DC2E6F37F84650BF96F9CC1ECB1FBF';\n",
    "\n",
    "# rseId='69DC2E6F37F84650BF96F9CC1ECB1FBF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e67fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_replicas = get_df_replicas(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afed63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File replicas at given rse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe119a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1a18160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_spark_session(yarn=True, verbose=False):\n",
    "#     \"\"\"Get or create the spark context and session.\n",
    "#     \"\"\"\n",
    "#     sc = SparkContext(appName='cms-monitoring-rucio-datasets-for-mongo')\n",
    "#     return SparkSession.builder.config(conf=sc._conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f8f08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_rses(spark):\n",
    "    \"\"\"Get pandas dataframe of RSES\n",
    "    \"\"\"\n",
    "    df_rses = spark.read.format(\"com.databricks.spark.avro\").load(HDFS_RUCIO_RSES) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('ID')))) \\\n",
    "        .withColumn('rse_tier', _split(col('RSE'), '_').getItem(0)) \\\n",
    "        .withColumn('rse_country', _split(col('RSE'), '_').getItem(1)) \\\n",
    "        .withColumn('rse_kind',\n",
    "                    when((col(\"rse\").endswith('Temp') | col(\"rse\").endswith('temp') | col(\"rse\").endswith('TEMP')),\n",
    "                         'temp')\n",
    "                    .when((col(\"rse\").endswith('Test') | col(\"rse\").endswith('test') | col(\"rse\").endswith('TEST')),\n",
    "                          'test')\n",
    "                    .otherwise('prod')\n",
    "                    ) \\\n",
    "        .select(['rse_id', 'RSE', 'RSE_TYPE', 'rse_tier', 'rse_country', 'rse_kind'])\n",
    "    return df_rses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2278acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rses = get_df_rses(spark)\n",
    "# df_rses.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c9eaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rses.filter(col('rse_kind') =='prod').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04fb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a944f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_replicas = get_df_replicas(spark)\n",
    "# df_replicas.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b09b71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_dids_files(spark):\n",
    "    \"\"\"Create spark dataframe for DIDS table by selecting only Files in Rucio DIDS table.\n",
    "\n",
    "    Filters:\n",
    "        - DELETED_AT not null\n",
    "        - HIDDEN = 0\n",
    "        - SCOPE = cms\n",
    "        - DID_TYPE = F\n",
    "\n",
    "    Columns selected:\n",
    "        - f_name: file name\n",
    "        - f_size_dids: represents size of a file in DIDS table\n",
    "        - dids_accessed_at: file last access time\n",
    "        - dids_created_at: file creation time\n",
    "    \"\"\"\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_DIDS) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .filter(col('HIDDEN') == '0') \\\n",
    "        .filter(col('SCOPE') == 'cms') \\\n",
    "        .filter(col('DID_TYPE') == 'F') \\\n",
    "        .withColumnRenamed('NAME', 'f_name') \\\n",
    "        .withColumnRenamed('ACCESSED_AT', 'dids_accessed_at') \\\n",
    "        .withColumnRenamed('CREATED_AT', 'dids_created_at') \\\n",
    "        .withColumn('f_size_dids', col('BYTES').cast(LongType())) \\\n",
    "        .select(['f_name', 'f_size_dids', 'dids_accessed_at', 'dids_created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba8995d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_ds_locks(spark):\n",
    "    \"\"\"Create dataset locks dataframe\"\"\"\n",
    "    return spark.read.format('avro').load(HDFS_RUCIO_DATASET_LOCKS) \\\n",
    "        .filter(col('SCOPE') == 'cms') \\\n",
    "        .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "        .withColumnRenamed('NAME', 'b_name_ds_locks') \\\n",
    "        .withColumnRenamed('ACCOUNT', 'account_ds_locks') \\\n",
    "        .select(['b_name_ds_locks', 'account_ds_locks', 'rse_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52068a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_dbs_f_d(spark):\n",
    "    \"\"\"Create a dataframe for FILE-DATASET membership/ownership map\n",
    "\n",
    "    Columns selected: f_name, dataset\n",
    "    \"\"\"\n",
    "    dbs_files = spark.read.format('avro').load(HDFS_DBS_FILES) \\\n",
    "        .withColumnRenamed('LOGICAL_FILE_NAME', 'f_name') \\\n",
    "        .withColumnRenamed('DATASET_ID', 'f_dataset_id') \\\n",
    "        .select(['f_name', 'f_dataset_id'])\n",
    "    dbs_datasets = spark.read.format('avro').load(HDFS_DBS_DATASETS)\n",
    "    \n",
    "    df_dbs_f_d = dbs_files.join(dbs_datasets, dbs_files.f_dataset_id == dbs_datasets.d_dataset_id, how='left') \\\n",
    "        .withColumnRenamed('f_dataset_id', 'dataset_id') \\\n",
    "        .withColumnRenamed('d_dataset', 'dataset') \\\n",
    "        .select(['dataset_id', 'f_name', 'dataset'])\n",
    "    return df_dbs_f_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04e9822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbs_datasets = spark.read.format('avro').load(HDFS_DBS_DATASETS)\n",
    "# dbs_datasets.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0066596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbs_files = spark.read.format('avro').load(HDFS_DBS_FILES)\n",
    "# dbs_files.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bd59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b0d8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_replicas_j_dids(df_replicas, df_dids_files):\n",
    "    \"\"\"Left join of df_replicas and df_dids_files to fill the RSE_ID, f_size and accessed_at, created_at for all files.\n",
    "\n",
    "    Be aware that there are 2 columns for each f_size, accessed_at, created_at\n",
    "    They will be combined in get_df_file_rse_ts_size\n",
    "\n",
    "    Columns:\n",
    "        comes from DID:       file, dids_accessed_at, dids_created_at, f_size_dids,\n",
    "        comes from REPLICAS:  file, rse_id, f_size_replicas, rep_accessed_at, rep_created_at\n",
    "   \"\"\"\n",
    "    return df_replicas.join(df_dids_files, ['f_name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3567ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_file_rse_ts_size(df_replicas_j_dids):\n",
    "    \"\"\"Combines columns to get filled and correct values from join of DIDS and REPLICAS\n",
    "\n",
    "    Firstly, REPLICAS size value will be used. If there are files with no size values, DIDS size values will be used:\n",
    "    see 'when' function order. For accessed_at and created_at, their max values will be got.\n",
    "\n",
    "    Columns: file, rse_id, accessed_at, f_size, created_at\n",
    "\n",
    "    df_file_rse_ts_size: files and their rse_id, size and access time are completed\n",
    "    \"\"\"\n",
    "\n",
    "    # f_size is not NULL, already verified.\n",
    "    # df_file_rse_ts_size.filter(col('f_size').isNull()).limit(5).toPandas()\n",
    "    return df_replicas_j_dids \\\n",
    "        .withColumn('f_size',\n",
    "                    when(col('f_size_replicas').isNotNull(), col('f_size_replicas'))\n",
    "                    .when(col('f_size_dids').isNotNull(), col('f_size_dids'))\n",
    "                    ) \\\n",
    "        .withColumn('accessed_at',\n",
    "                    greatest(col('dids_accessed_at'), col('rep_accessed_at'))\n",
    "                    ) \\\n",
    "        .withColumn('created_at',\n",
    "                    greatest(col('dids_created_at'), col('rep_created_at'))\n",
    "                    ) \\\n",
    "        .select(['f_name', 'rse_id', 'accessed_at', 'f_size', 'created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd2b248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_dataset_file_rse_ts_size(df_file_rse_ts_size, df_dbs_f_d):\n",
    "    \"\"\" Left join df_file_rse_ts_size and df_dbs_f_d to get dataset names of files.\n",
    "\n",
    "    In short: adds 'dataset' names to 'df_file_rse_ts_size' dataframe by joining DBS tables\n",
    "\n",
    "    Columns: block(from df_contents_f_to_b), file, rse_id, accessed_at, f_size\n",
    "    \"\"\"\n",
    "    df_dataset_file_rse_ts_size = df_file_rse_ts_size \\\n",
    "        .join(df_dbs_f_d, ['f_name'], how='left') \\\n",
    "        .fillna(\"UnknownDatasetNameOfFiles_MonitoringTag\", subset=['dataset']) \\\n",
    "        .select(['dataset_id', 'dataset', 'f_name', 'rse_id', 'accessed_at', 'created_at', 'f_size'])\n",
    "\n",
    "    # f_c = df_dataset_file_rse_ts_size.select('f_name').distinct().count()\n",
    "    # f_w_no_dataset_c = df_dataset_file_rse_ts_size.filter(col('dataset').isNull()).select('f_name').distinct().count()\n",
    "    # print('Distinct file count:', f_c)\n",
    "    # print('Files with no dataset name count:', f_w_no_dataset_c)\n",
    "    # print('% of null dataset name in all files:', round(f_w_no_dataset_c / f_c, 3) * 100)\n",
    "\n",
    "    return df_dataset_file_rse_ts_size\n",
    "\n",
    "\n",
    "def get_df_enr_with_rse_info(df_dataset_file_rse_ts_size, df_rses):\n",
    "    \"\"\"Add RSE type, name, kind, tier, country by joining RSE ID\"\"\"\n",
    "    return df_dataset_file_rse_ts_size.join(df_rses, ['rse_id'], how='left') \\\n",
    "        .select(['dataset_id', 'dataset', 'f_name', 'rse_id', 'accessed_at', 'created_at', 'f_size',\n",
    "                 'RSE', 'RSE_TYPE', 'rse_tier', 'rse_country', 'rse_kind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3cc136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Main dataset functions\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def get_df_sub_rse_details(df_enr_with_rse_info):\n",
    "    \"\"\"Get dataframe of datasets that are not read since N months for sub details htmls\n",
    "\n",
    "    Group by 'dataset' and 'rse_id' of get_df_dataset_file_rse_ts_size\n",
    "\n",
    "    Filters:\n",
    "        - If a dataset contains EVEN a single file with null accessed_at, filter out\n",
    "\n",
    "    Access time filter logic:\n",
    "        - If 'last_access_time_of_dataset_in_all_rses' is less than 'n_months_filter', ...\n",
    "          ... set 'is_not_read_since_{n_months_filter}_months' column as True\n",
    "\n",
    "    Columns:\n",
    "        - 'dataset_size_in_rse_gb'\n",
    "                Total size of a Dataset in an RSE.\n",
    "                Produced by summing up datasets' all files in that RSE.\n",
    "        - 'last_access_time_of_dataset_in_rse'\n",
    "                Last access time of a Dataset in an RSE.\n",
    "                Produced by getting max `accessed_at`(represents single file's access time) of a dataset in an RSE.\n",
    "        - '#files_with_null_access_time_of_dataset_in_rse'\n",
    "                Number of files count, which have NULL `accessed_at` values, of a Dataset in an RSE.\n",
    "                This is important to know to filter out if there is any NULL `accessed_at` value of a Dataset.\n",
    "        - '#files_of_dataset_in_rse'\n",
    "                Number of files count of a Dataset in an RSE\n",
    "        - '#distinct_files_of_dataset_in_rse'\n",
    "                Number of unique files count of dataset in an RSE\n",
    "\n",
    "    df_main_datasets_and_rses: RSE name, dataset and their size and access time calculations\n",
    "    \"\"\"\n",
    "    # Get RSE ID:NAME map\n",
    "    # rses_id_name_map = dict(df_pd_rses[['replica_rse_id', 'rse']].values)\n",
    "    # rses_id_type_map = dict(df_pd_rses[['replica_rse_id', 'rse_type']].values)\n",
    "    # rses_id_tier_map = dict(df_pd_rses[['replica_rse_id', 'rse_tier']].values)\n",
    "    # rses_id_country_map = dict(df_pd_rses[['replica_rse_id', 'rse_country']].values)\n",
    "    # rses_id_kind_map = dict(df_pd_rses[['replica_rse_id', 'rse_kind']].values)\n",
    "    # .replace(rses_id_name_map, subset=['rse_id']) \\\n",
    "    # , 'rse_tier', 'rse_country', 'rse_kind',\n",
    "    return df_enr_with_rse_info \\\n",
    "        .groupby(['rse_id', 'dataset']) \\\n",
    "        .agg(_sum(col('f_size')).alias('SizeInRseBytes'),\n",
    "             _max(col('accessed_at')).alias('LastAccessInRse'),\n",
    "             _count(lit(1)).alias('FileCnt'),\n",
    "             _sum(when(col('accessed_at').isNull(), 0).otherwise(1)).alias('AccessedFileCnt'),\n",
    "             first(col('dataset_id')).alias('dataset_id'),\n",
    "             first(col('RSE_TYPE')).alias('RseType'),\n",
    "             first(col('RSE')).alias('RSE'),\n",
    "             first(col('rse_tier')).alias('rse_tier'),\n",
    "             first(col('rse_country')).alias('rse_country'),\n",
    "             first(col('rse_kind')).alias('rse_kind'),\n",
    "             ) \\\n",
    "        .withColumnRenamed('dataset', 'Dataset') \\\n",
    "        .select(['dataset_id', 'RseType', 'RSE', 'Dataset', 'SizeInRseBytes',\n",
    "                 'LastAccessInRse', 'FileCnt', 'AccessedFileCnt', ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "725c020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_main_datasets(df_sub_rse_details):\n",
    "    \"\"\"Get dataframe of datasets not read since N months for main htmls.\n",
    "\n",
    "    Get last access of dataframe in all RSE(s)\n",
    "    \"\"\"\n",
    "    # Order of the select is important\n",
    "    return df_sub_rse_details \\\n",
    "        .groupby(['RseType', 'Dataset']) \\\n",
    "        .agg((_max(col('SizeInRseBytes')) / TB_DENOMINATOR).cast(DecimalType(20, FLT_N_TB_DGTS)).alias('MaxTB'),\n",
    "             (_min(col('SizeInRseBytes')) / TB_DENOMINATOR).cast(DecimalType(20, FLT_N_TB_DGTS)).alias('MinTB'),\n",
    "             (_avg(col('SizeInRseBytes')) / TB_DENOMINATOR).cast(DecimalType(20, FLT_N_TB_DGTS)).alias('AvgTB'),\n",
    "             (_sum(col('SizeInRseBytes')) / TB_DENOMINATOR).cast(DecimalType(20, FLT_N_TB_DGTS)).alias('SumTB'),\n",
    "             _max(col('LastAccessInRse')).alias('LastAccessMs'),\n",
    "             concat_ws(', ', collect_list('RSE')).alias('RSEs'),\n",
    "             first(col('dataset_id')).cast(LongType()).alias('Id'),\n",
    "             ) \\\n",
    "        .withColumn('LastAccess', from_unixtime(col(\"LastAccessMs\") / 1000, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \\\n",
    "        .select(['Id', 'RseType', 'Dataset', 'LastAccess', 'LastAccessMs',\n",
    "                 'MaxTB', 'MinTB', 'AvgTB', 'SumTB',\n",
    "                 'RSEs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd546e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_dbs_agg_blocks(spark, df_replicas):\n",
    "    dbs_files = spark.read.format('avro').load(HDFS_DBS_FILES) \\\n",
    "        .withColumnRenamed('LOGICAL_FILE_NAME', 'f_dbs_name') \\\n",
    "        .withColumnRenamed('DATASET_ID', 'f_dataset_id') \\\n",
    "        .withColumnRenamed('BLOCK_ID', 'f_block_id') \\\n",
    "        .withColumnRenamed('FILE_SIZE', 'f_dbs_size') \\\n",
    "        .select(['f_dbs_name', 'f_dataset_id', 'f_block_id', 'f_dbs_size'])\n",
    "\n",
    "    df_files = df_replicas.join(dbs_files, df_replicas.f_name == dbs_files.f_dbs_name, how='left') \\\n",
    "        .select(['f_name', 'f_size_replicas', 'f_dataset_id', 'f_block_id'])\n",
    "\n",
    "    dbs_blocks = spark.read.format('avro').load(HDFS_DBS_BLOCKS) \\\n",
    "        .withColumnRenamed('DATASET_ID', 'b_dataset_id') \\\n",
    "        .withColumnRenamed('BLOCK_ID', 'b_block_id') \\\n",
    "        .withColumnRenamed('BLOCK_NAME', 'b_name') \\\n",
    "        .withColumnRenamed('FILE_COUNT', 'b_file_cnt') \\\n",
    "        .select(['b_name', 'b_block_id', 'b_dataset_id', 'b_file_cnt'])\n",
    "\n",
    "    df_dbs_b_agg = df_files.join(dbs_blocks, df_files.f_block_id == dbs_blocks.b_block_id, how='left') \\\n",
    "        .groupby('b_block_id') \\\n",
    "        .agg(_sum('f_size_replicas').alias('b_size'),\n",
    "             first('b_file_cnt').alias('b_file_cnt'),\n",
    "             first('b_name').alias('b_name'),\n",
    "             first('b_dataset_id').alias('b_dataset_id'),\n",
    "             ) \\\n",
    "        .select(['b_block_id', 'b_name', 'b_dataset_id', 'b_size', 'b_file_cnt'])\n",
    "\n",
    "    return df_dbs_b_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3edd6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_dbs_main_ds_size(spark):\n",
    "    dbs_files = spark.read.format('avro').load(HDFS_DBS_FILES) \\\n",
    "        .withColumnRenamed('LOGICAL_FILE_NAME', 'f_name') \\\n",
    "        .withColumnRenamed('DATASET_ID', 'f_dataset_id') \\\n",
    "        .withColumnRenamed('BLOCK_ID', 'f_block_id') \\\n",
    "        .withColumnRenamed('FILE_SIZE', 'f_dbs_size') \\\n",
    "        .select(['f_name', 'f_dataset_id', 'f_block_id', 'f_dbs_size'])\n",
    "    dbs_datasets = spark.read.format('avro').load(HDFS_DBS_DATASETS) \\\n",
    "        .withColumnRenamed('DATASET_ID', 'd_dataset_id') \\\n",
    "        .withColumnRenamed('DATASET', 'd_dataset') \\\n",
    "        .select(['d_dataset_id', 'd_dataset'])\n",
    "    df_dbs_d_agg = dbs_files.join(dbs_datasets, dbs_files.f_dataset_id == dbs_datasets.d_dataset_id, how='left') \\\n",
    "        .withColumnRenamed('f_dataset_id', 'dataset_id') \\\n",
    "        .groupby('dataset_id') \\\n",
    "        .agg(_sum('f_dbs_size').alias('d_dbs_size'),\n",
    "             first('d_dataset').alias('d_name')\n",
    "             ) \\\n",
    "        .select(['dataset_id', 'd_name', 'd_dbs_size'])\n",
    "    return df_dbs_d_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2548cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def main(hdfs_out_dir):\n",
    "# #     \"\"\"Main function that run Spark dataframe creations and save results to HDFS directory as JSON lines\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     # HDFS output file format. If you change, please modify bin/cron4rucio_ds_mongo.sh accordingly.\n",
    "# #     write_format = 'json'\n",
    "# #     write_mode = 'overwrite'\n",
    "\n",
    "# #     spark = get_spark_session()\n",
    "# #     # Set TZ as UTC. Also set in the spark-submit confs.\n",
    "# #     spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "#     # The reason that we have lots of functions that returns PySpark dataframes is mainly for code readability.\n",
    "# df_rses = get_df_rses(spark)\n",
    "# df_dbs_f_d = get_df_dbs_f_d(spark)\n",
    "# df_replicas = get_df_replicas(spark)\n",
    "# df_dids_files = get_df_dids_files(spark)\n",
    "# df_replicas_j_dids = get_df_replicas_j_dids(df_replicas, df_dids_files)\n",
    "# df_file_rse_ts_size = get_df_file_rse_ts_size(df_replicas_j_dids)\n",
    "# df_dataset_file_rse_ts_size = get_df_dataset_file_rse_ts_size(df_file_rse_ts_size, df_dbs_f_d)\n",
    "# df_enr_with_rse_info = get_df_enr_with_rse_info(df_dataset_file_rse_ts_size, df_rses)\n",
    "# df_sub_rse_details = get_df_sub_rse_details(df_enr_with_rse_info)\n",
    "# df_main_datasets = get_df_main_datasets(df_sub_rse_details)\n",
    "\n",
    "# #     df_main_datasets.write.save(path=hdfs_out_dir, format=write_format, mode=write_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07718c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "608a5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_main_datasets.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b7f1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub_rse_details.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7203b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", \"rchauhan\").option(\"collection\", \"new_collection\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b26e95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98dba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.0.1"
    },
    {
     "name": "spark.jars",
     "value": "mongo-spark-connector_2.12-3.0.2.jar, bson-4.7.0.jar, mongo-java-driver-3.12.11.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
